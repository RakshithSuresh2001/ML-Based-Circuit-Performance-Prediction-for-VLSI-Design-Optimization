#ONE
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import numpy as np

print("ğŸ¯ Preparing data for ML training...\n")

# Select features for modeling
feature_columns = [
    'Area_um2', 
    'Input_Cap_pF', 
    'Drive_Strength',
    'Normalized_Area',
    'Delay_per_Area',
    'Log_Area',
    'Area_x_Cap'
]

# Target variable
target = 'Avg_Delay_ns'

# Prepare X (features) and y (target)
X = df_engineered[feature_columns].copy()
y = df_engineered[target].copy()

# Remove any rows with NaN or inf
mask = ~(X.isnull().any(axis=1) | np.isinf(X).any(axis=1) | y.isnull() | np.isinf(y))
X = X[mask]
y = y[mask]

print(f"âœ… Data prepared!")
print(f"ğŸ“Š Samples: {len(X)}")
print(f"ğŸ“Š Features: {len(feature_columns)}")
print(f"ğŸ“Š Target: {target}")

#TWO
# Split data (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print(f"\nğŸ“Š Train set: {len(X_train)} samples")
print(f"ğŸ“Š Test set: {len(X_test)} samples")

# Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("âœ… Features scaled!")

#THREE
print("\n" + "="*60)
print("ğŸ¤– TRAINING ML MODELS")
print("="*60 + "\n")

# Dictionary to store models and results
models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(alpha=1.0),
    'Lasso Regression': Lasso(alpha=0.01),
    'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42),
    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42)
}

results = []

for name, model in models.items():
    print(f"ğŸ”„ Training {name}...")
    
    # Train
    model.fit(X_train_scaled, y_train)
    
    # Predict
    y_train_pred = model.predict(X_train_scaled)
    y_test_pred = model.predict(X_test_scaled)
    
    # Evaluate
    train_r2 = r2_score(y_train, y_train_pred)
    test_r2 = r2_score(y_test, y_test_pred)
    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
    test_mae = mean_absolute_error(y_test, y_test_pred)
    
    # Convert to picoseconds for readability
    test_rmse_ps = test_rmse * 1e12
    test_mae_ps = test_mae * 1e12
    
    results.append({
        'Model': name,
        'Train_R2': train_r2,
        'Test_R2': test_r2,
        'RMSE_ps': test_rmse_ps,
        'MAE_ps': test_mae_ps,
        'model_object': model
    })
    
    print(f"  âœ… Train RÂ²: {train_r2:.4f}")
    print(f"  âœ… Test RÂ²: {test_r2:.4f}")
    print(f"  âœ… RMSE: {test_rmse_ps:.2f} ps")
    print(f"  âœ… MAE: {test_mae_ps:.2f} ps\n")

# Create results DataFrame
results_df = pd.DataFrame(results)
results_df = results_df.sort_values('Test_R2', ascending=False)

print("\n" + "="*60)
print("ğŸ“Š MODEL COMPARISON")
print("="*60)
print(results_df[['Model', 'Train_R2', 'Test_R2', 'RMSE_ps', 'MAE_ps']].to_string(index=False))

#FOUR
# Get best model
best_model_name = results_df.iloc[0]['Model']
best_model = results_df.iloc[0]['model_object']

print("\n" + "="*60)
print(f"ğŸ† BEST MODEL: {best_model_name}")
print("="*60)

# Make predictions with best model
y_test_pred = best_model.predict(X_test_scaled)

# Detailed metrics
print(f"\nğŸ“Š Performance Metrics:")
print(f"  RÂ² Score: {r2_score(y_test, y_test_pred):.4f}")
print(f"  RMSE: {np.sqrt(mean_squared_error(y_test, y_test_pred))*1e12:.2f} ps")
print(f"  MAE: {mean_absolute_error(y_test, y_test_pred)*1e12:.2f} ps")
print(f"  Max Error: {np.max(np.abs(y_test - y_test_pred))*1e12:.2f} ps")

# Percentage error
mape = np.mean(np.abs((y_test - y_test_pred) / y_test)) * 100
print(f"  MAPE: {mape:.2f}%")

#FIVE
# Create prediction visualization
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Convert to picoseconds for plotting
y_test_ps = y_test * 1e12
y_test_pred_ps = y_test_pred * 1e12

# 1. Scatter plot: Predicted vs Actual
axes[0].scatter(y_test_ps, y_test_pred_ps, alpha=0.6, s=50)
axes[0].plot([y_test_ps.min(), y_test_ps.max()], 
             [y_test_ps.min(), y_test_ps.max()], 
             'r--', lw=2, label='Perfect Prediction')
axes[0].set_xlabel('Actual Delay (ps)', fontsize=12)
axes[0].set_ylabel('Predicted Delay (ps)', fontsize=12)
axes[0].set_title(f'{best_model_name}: Predicted vs Actual\nRÂ² = {r2_score(y_test, y_test_pred):.4f}', 
                  fontsize=14, fontweight='bold')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# 2. Residual plot (errors)
residuals_ps = (y_test - y_test_pred) * 1e12
axes[1].scatter(y_test_pred_ps, residuals_ps, alpha=0.6, s=50, color='coral')
axes[1].axhline(y=0, color='red', linestyle='--', lw=2)
axes[1].set_xlabel('Predicted Delay (ps)', fontsize=12)
axes[1].set_ylabel('Residual (ps)', fontsize=12)
axes[1].set_title('Residual Plot', fontsize=14, fontweight='bold')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("ğŸ“Š Prediction analysis complete!")